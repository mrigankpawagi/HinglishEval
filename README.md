# HinglishEval
Are our Large Language Models good at code-generation when prompted in native languages?

## Prompting in Hinglish
We used GPT-4 to translate prompts in HumanEval to _Hinglish_ and manually verified and fixed these translations. The new dataset, __HinglishEval__ is available as a JSON file (`HinglishEval.json`) in the repository.

### Why Hinglish?
Hindi is the most spoken language apart from English in India. But spoken Hindi is never pure Hindi but mixture of English words. If a regular man would prompt ChatGPT, Gemini or any other LLM, he would prefer typing Hinglish than pure Hindi or English. This benchmark deals with code generated by different code generating LLM's when prompted in Hinglish. 

## Results
We evaluated 18 models on the HinglishEval dataset, at temperature 0 (greedy decoding).

| Model | Pass@1 | 
| --- | --- |
| GPT 4 | 79.27 |
| GPT 3.5 Turbo | 58.54 |
| Gemma 7B | 31.71 |
| Gemma 2B | 17.68 |
| Codegen 6B Mono | 15.24 |
| Codegen 2B Mono | 9.76 |
| Codegen 6B Multi | 8.54 |
| Santacoder | 6.71 |
| Codegen-2 1B | 4.88 |
| Codegen 6B NL | 3.66 |
| Codegen 350M Mono | 3.05 |
| Codegen 350M Multi | 3.05 |
| Codegen 2B NL | 2.44 |
| Polycoder 2.7B | 1.83 |
| Codegen 2B Multi | 1.83 |
| Polycoder 0.4B | 0.00 |
| Polycoder 160M | 0.00 |
| Codegen 350M NL | 0.00 |

## Contributions and Usage
1. Translated HumanEval to Hinglish
We used GPT-4 to translate the docstrings from `HumanEval.json` and manually eyeballed through all the generated Hinglish translations for minor adjustments and errors, making sure it sounded like spoken Hinglish. Only the docstrings were changed, the function name, doctests and strings used in examples were not translated.

2. Released Code Samples for 18 models
The translated docstrings were feeded to the 18 models and ran once. The collection of 164 codes for each model, had unwanted code lines like the main function, print statements, halucinated texts after the function had completed, etc. A sanitization function was made to collect the function and its imports only. Both sets of codes are collected as `unsanitized` and `sanitized` codes.

3. Evaluated 18 models on HinglishEval
We ran the tests from HumanEval and few self made doctests on each code. Based on the number of correct codes each model had predicted, ratings out of 100 to each model was given. 

## Future Work
1. The interpretation of the ratings based on each model
2. Critically analyse the codes of models with high scores and low scores
3. Think about what could have been done to increase the efficiency of models
and more.
