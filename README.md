# HinglishEval
Artifact for **HinglishEval: Evaluating the Effectiveness of Code-generation Models on Hinglish Prompts**.

## Prompting in Hinglish
To automate the task to translation of the dataset to Hinglish, we have used a primary trusted model - GPT-4 to translate the prompts in OpenAI's [HumanEval Benchmark](https://github.com/openai/human-eval) to _Hinglish_ . This was followed by manual verification of the translations. The benchmark - __HinglishEval__ is available as a JSON file in [`HinglishEval.json`](https://github.com/mrigankpawagi/HinglishEval/blob/main/HinglishEval.json).

### Why Hinglish?
Hindi is one of the most widely spoken languages in the world, and the most widely spoen in India. A majority of the population in India does not speak English as their first language, and therefore language models that can understand prompts in native languages are important for wider accessibility. Hinglish is a blend of Hindi and English, with frequent usage of English words in sentences with standard Hindi grammar. This is not representative of everyday spoken Hindi for most people, but is rather common in coversations involving technical language, especially in the context of programming.

Therefore it is most natural for Hindi speaking users to prompt LLMs in Hinglish when they want to generate code, or ask for help with programming in general (like explanations or debugging). This benchmark is an attempt to understand how well LLMs can understand and generate code when prompted in such a language.

## Contributions and Usage

### The HinglishEval Benchmark

The HinglishEval benchmark contains all the problems in the HumnaEval benchmark with their prompts translated to Hinglish. This translation does not modify the function signature or the formats of the doctests and is limited to the purpose statement. THis statement refers to the __docstring__ in specific to Python functions. The translations were manually verified and corrected to ensure that they sound like idiomatic Hinglish. 

### Code Samples: Model Completions for HinglishEval
We have released code completions generated by 18 different models on the HinglishEval prompts. These completions are available in two forms:

- **Unsanitized Completions**: Found in the `samples/unsanitized` directory, these are raw outputs generated by the models without any post-processing.

- **Sanitized Completions**: Found in the `samples/sanitized directory`.
These outputs are processed to:
- Retain only the requested function definitions.
- Remove any extraneous or irrelevant text.
- Sanitization ensures that only the code relevant to the problem is considered during evaluation.

### Evaluation of Models on HinglishEval
We evaluated 18 models on the HinglishEval dataset using two methodologies:

- Pass@1 Metric:
This metric evaluates the correctness of model-generated code on the first attempt. All evaluations were conducted using a temperature setting of 0 (greedy decoding).

- Item Response Theory (IRT):
IRT is a statistical framework used to evaluate both the difficulty of problems and the capabilities of models. It provides deeper insights into:

**Problem Parameters**:
- Difficulty (β): Measures how challenging a problem is.
- Discrimination (α): Measures how well a problem differentiates between high- and low-performing models.
- User Parameters:
Represents the latent abilities (or competencies) of the models.